{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network #\n",
    "Full implementation of artificial neurons and MLPs without the use of in built functions or related libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports ##\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Mathplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                         # For Array Operations\n",
    "import pandas as pd                        # For DataFrame Operations\n",
    "import matplotlib.pyplot as plt            # For Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Global Functions ###\n",
    "| Function Name          | Use                                |\n",
    "| :--------------------: | :--------------------------------: |\n",
    "|activation_function     | Defines few activation functions   |\n",
    "|activation_derivative   | Pre-evaluated derivatives          |\n",
    "|loss_function           | Defines few loss functions         |\n",
    "|loss_derivative         | Pre-evaluated derivatives          |\n",
    "\n",
    "\n",
    "**Available activation functions :**\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "* ReLU\n",
    "* Leaky ReLU\n",
    "* Softmax\n",
    "\n",
    "\n",
    "**Available Loss functions :**\n",
    "* MSE\n",
    "* Binary Cross Entropy\n",
    "* Categorical Cross Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the activation functions and their derivatives\n",
    "def activation_function(function_name):\n",
    "    try:                                                                  \n",
    "        if (function_name.lower() == 'sigmoid'):\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif (function_name.lower() == 'tanh'):\n",
    "            return lambda x: np.tanh(x)\n",
    "        elif (function_name.lower() == 'relu'):\n",
    "            return lambda x: np.maximum(0, x)   \n",
    "        elif (function_name.lower() == 'leaky_relu'):\n",
    "            return lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "        elif (function_name.lower() == 'softmax'):\n",
    "            return lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding activation function: {e}\")\n",
    "        return None\n",
    "        \n",
    "def activation_derivative(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'sigmoid'):\n",
    "            return lambda x: x * (1 - x)\n",
    "        elif (function_name.lower() == 'tanh'):\n",
    "            return lambda x: 1 - np.tanh(x) ** 2\n",
    "        elif (function_name.lower() == 'relu'):\n",
    "            return lambda x: np.where(x > 0, 1, 0)\n",
    "        elif (function_name.lower() == 'leaky_relu'):\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif (function_name.lower() == 'softmax'):\n",
    "            return lambda x: x * (1 - x)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding activation derivative function: {e}\")\n",
    "        return None\n",
    "        \n",
    "# Defining the loss functions and their derivatives      \n",
    "def loss_function(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'mean_squared_error'):\n",
    "            return lambda y_true, y_pred: np.mean((y_true - y_pred) ** 2)\n",
    "        elif (function_name.lower() == 'binary_crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        elif (function_name.lower() == 'categorical_crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.sum(y_true * np.log(y_pred), axis=1).mean()\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding loss function: {e}\")\n",
    "        return None\n",
    "\n",
    "def loss_derivative(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'mean_squared_error'):\n",
    "            return lambda y_true, y_pred: 2 * (y_pred - y_true) / y_true.size\n",
    "        elif (function_name.lower() == 'binary_crossentropy'):\n",
    "            return lambda y_true, y_pred: -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        elif (function_name.lower() == 'categorical_crossentropy'):\n",
    "            return lambda y_true, y_pred: y_pred - y_true\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding loss derivative function: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class ###\n",
    "To utilise a deep network of neuron layers, we can define a layer class with variable parameters like, number of neurons in the layer (inputs) and the number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # Initializing the layer with weights and biases with a default activation function (Sigmoid)\n",
    "    def __init__(self, input_size, output_size, function_name='sigmoid'):\n",
    "        self.W = np.random.randn(output_size, input_size) * 0.01    # Initializing weights with small random values\n",
    "        self.B = np.zeros((output_size, 1))                         # Initializing biases with zeros\n",
    "        self.function_name = function_name                          # Storing the activation function name\n",
    "        \n",
    "    # Defining the feed forward function for the layer\n",
    "    def feed_forward(self, X):\n",
    "        self.input = X                                             # Storing the input for back propagation\n",
    "        self.Z = np.dot(self.W, X) + self.B                         # Simply implementing the formula Z = WX + B\n",
    "        self.A = activation_function(self.function_name)(self.Z)    # Applying the activation function to Z to get A\n",
    "        return self.A\n",
    "    \n",
    "    # Defining the back propagation function for the layer\n",
    "    def back_propagation(self, dA):\n",
    "        m = self.input.shape[1]                                              # To utilise batch processing (Averaging over m samples per batch)\n",
    "        dL = dA * activation_derivative(self.function_name)(self.A) # Using the chain rule to calculate the derivative of the loss with respect to Z - (dL = dL/dZ)\n",
    "        self.dW = np.dot(dL, self.input.T) / m                               # Gradient of the loss with respect to W - (dL/dW = dL/dZ * dZ/dW)\n",
    "        self.dB = np.sum(dL, axis=1, keepdims=True) / m             # Gradient of the loss with respect to B - (dL/dB = dL/dZ * dZ/dB)\n",
    "        dX = np.dot(self.W.T, dL)                                   # Gradient of the loss with respect to Input - (dL/dX = dL/dZ * dZ/dX)          \n",
    "        \n",
    "        return dX                                                   # Return the gradient of the loss with respect to the input for the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the Neural Network ###\n",
    "Here, we pass in a random input data and define the ground truth for the model to train on\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = np.array([[1,2], [4, 6], [7, 8], [2, 10], [5, 1]])  # Input data with shape (5, 2)\n",
    "true_output = np.array([[0, 0], [1, 1]])\n",
    "print(input_data.shape, true_output.shape)  # Print the shape of input and output data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We choose a 3 layer architecture, each with sigmoid activation along with binary cross entropy loss\n",
    "Through the following implementation we try:\n",
    "\n",
    "* A simple Neural Network with 1 Hidden Layer\n",
    "* binary classification of input from 5 neurons\n",
    "* Training over two iterations\n",
    "* Plotting the loss function to assess the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Layer1 = Layer(5,4, 'sigmoid') # Input layer\n",
    "Layer2 = Layer(4,3, 'sigmoid') # Hidden Layer\n",
    "Layer3 = Layer(3,2, 'sigmoid') # Output Layer\n",
    "\n",
    "# Defining a Neural Network\n",
    "\n",
    "L1 = Layer1.feed_forward(input_data)    # Feed forward the input data through the first layer\n",
    "L2 = Layer2.feed_forward(L1)            # Feed forward the output of the first layer to the second layer\n",
    "L3 = Layer3.feed_forward(L2)            # Feed forward the output of the second layer to the third layer\n",
    "print()\n",
    "print(\"First Prediction:\")\n",
    "print(L3)\n",
    "print()\n",
    "\n",
    "# Backpropagation\n",
    "losses = []  # List to store loss values\n",
    "losses.append(loss_function('binary_crossentropy')(true_output, L3))  # Calculate the loss value using binary crossentropy\n",
    "print(\"Loss Value [1] : {loss:.4f}\".format(loss = losses[0]))\n",
    "print()\n",
    "dL3 = loss_derivative('binary_crossentropy')(true_output, L3)       # Calculate the derivative of the loss with respect to the output\n",
    "dL2 = Layer3.back_propagation(dL3)                              # Back propagate the error to the second layer\n",
    "dL1 = Layer2.back_propagation(dL2)                              # Back propagate the error to the first layer\n",
    "dI = Layer1.back_propagation(dL1)                       # Back propagate the error to the input layer\n",
    "print(\"Backpropagation:\")\n",
    "print(dL1)                                                          # Print the gradient of the loss with respect to the input of the first layer\n",
    "print(dL2)                                                          # Print the gradient of the loss with respect to the input of the second layer\n",
    "print(dL3)                                                          # Print the gradient of the loss with respect to the output of the third layer\n",
    "print()\n",
    "print(\"Weights and Biases:\")\n",
    "print(\"Layer 1 Weights:\")\n",
    "print(Layer1.W)                     # Print the weights of the first layer\n",
    "print(\"Layer 1 Biases:\")\n",
    "print(Layer1.B)                     # Print the biases of the first layer\n",
    "print(\"Layer 2 Weights:\")\n",
    "print(Layer2.W)                     # Print the weights of the second layer\n",
    "print(\"Layer 2 Biases:\")\n",
    "print(Layer2.B)                     # Print the biases of the second layer\n",
    "print(\"Layer 3 Weights:\")\n",
    "print(Layer3.W)                     # Print the weights of the third layer\n",
    "print(\"Layer 3 Biases:\")\n",
    "print(Layer3.B)                     # Print the biases of the third layer\n",
    "print()\n",
    "\n",
    "# Updating Weights and Biases\n",
    "Layer1.W -= Layer1.dW   # Update the weights of the first layer\n",
    "Layer1.B -= Layer1.dB   # Update the biases of the first layer\n",
    "Layer2.W -= Layer2.dW   # Update the weights of the second layer\n",
    "Layer2.B -= Layer2.dB   # Update the biases of the second layer\n",
    "Layer3.W -= Layer3.dW   # Update the weights of the third layer\n",
    "Layer3.B -= Layer3.dB   # Update the biases of the third layer\n",
    "print(\"Updated Weights and Biases:\")\n",
    "print(\"Layer 1 Weights:\")\n",
    "print(Layer1.W)                     # Print the weights of the first layer\n",
    "print(\"Layer 1 Biases:\")\n",
    "print(Layer1.B)                     # Print the biases of the first layer\n",
    "print(\"Layer 2 Weights:\")\n",
    "print(Layer2.W)                     # Print the weights of the second layer\n",
    "print(\"Layer 2 Biases:\")\n",
    "print(Layer2.B)                     # Print the biases of the second layer\n",
    "print(\"Layer 3 Weights:\")\n",
    "print(Layer3.W)                     # Print the weights of the third layer\n",
    "print(\"Layer 3 Biases:\")\n",
    "print(Layer3.B)                     # Print the biases of the third layer\n",
    "print()\n",
    "print(\"Second Prediction:\")\n",
    "L1 = Layer1.feed_forward(input_data)    # Feed forward the input data through the first layer\n",
    "L2 = Layer2.feed_forward(L1)            # Feed forward the output of the first layer to the second layer\n",
    "L3 = Layer3.feed_forward(L2)            # Feed forward the output of the second layer to the third layer\n",
    "print(L3)                               # Print the output of the third layer after updating the weights and biases\n",
    "print()\n",
    "losses.append(loss_function('binary_crossentropy')(true_output, L3))  # Calculate the loss value using binary crossentropy\n",
    "print(\"Loss Value [2] : {loss:.4f}\".format(loss = losses[1]))  # Calculate the loss value using binary crossentropy\n",
    "print()\n",
    "\n",
    "#Plotting the loss values\n",
    "plt.plot([1,2], losses)                 # Plot the loss values\n",
    "plt.title('Loss Value vs Iterations')   # Set the title of the plot\n",
    "plt.xlabel('Iterations')                # Set the x-axis label\n",
    "plt.ylabel('Loss Value')                # Set the y-axis label\n",
    "plt.grid()                              # Add grid lines to the plot\n",
    "plt.show()                              # Show the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network & Data Definition ##\n",
    "Now, we will try to create a Neural Network Class to be organise and train the model over custom architectures.\n",
    "\n",
    "For this project, we will utilise the MNIST number dataset and test our model's architecture over the classification metric.\n",
    "\n",
    "This part will cover two major steps:\n",
    "|S.No. | Step | Use |\n",
    "| ---- | ---- | --- |\n",
    "| 1.   | Data Frame Definition | To process the images in batches and parse input to the input layer|\n",
    "| 2.   | Neural Network Class  | To organise the code and implement the training steps over loop  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 1 ###\n",
    "<h4> (A) Creating a Data-Frame and functions for input handling <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining an IDX file reader for the MNIST dataset\n",
    "import struct\n",
    "\n",
    "\n",
    "# Image Loader\n",
    "def load_idx_images(filename):\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        # The first 4 bytes contain the magic number\n",
    "        magic = struct.unpack('>I', f.read(4))[0]\n",
    "        # The next 4 bytes contain the number of images\n",
    "        num_images = struct.unpack('>I', f.read(4))[0]\n",
    "        # Next, number of rows and columns\n",
    "        num_rows = struct.unpack('>I', f.read(4))[0]\n",
    "        num_cols = struct.unpack('>I', f.read(4))[0]\n",
    "        # Read the rest of the data:\n",
    "        # Each pixel is stored as an unsigned byte.\n",
    "        images = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        images = images.reshape(num_images, num_rows*num_cols)\n",
    "    return images\n",
    "\n",
    "# Label Loader\n",
    "def load_idx_labels(filename):\n",
    "    \n",
    "    with open(filename, 'rb') as f:\n",
    "        magic = struct.unpack('>I', f.read(4))[0]\n",
    "        num_labels = struct.unpack('>I', f.read(4))[0]\n",
    "        labels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "    return labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4> (B) Loading the Dataset <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_idx_images(r'D:\\Coding\\DSGxRMS\\Artificial Neural Network\\Dataset\\train-images.idx3-ubyte')  # Load the MNIST training images\n",
    "labels = load_idx_labels(r'D:\\Coding\\DSGxRMS\\Artificial Neural Network\\Dataset\\train-labels.idx1-ubyte')  # Load the MNIST training labels\n",
    "\n",
    "# Creating the Dataframe\n",
    "df = pd.DataFrame(images)  # Create a DataFrame from the images\n",
    "df['label'] = labels  # Add the labels to the DataFrame\n",
    "\n",
    "print(df.head())  # Print the first 5 rows of the DataFrame\n",
    "print(df.shape)  # Print the shape of the DataFrame\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step - 2 ###\n",
    "<h4> Defining the Neural Network <h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, epochs = 1000, batch_size = 64, learning_rate = 0.01):\n",
    "        \n",
    "        # Initializing the neural network with a list of layers\n",
    "        \n",
    "        self.layers= []                      # List to store the layers of the neural network\n",
    "        Layer1 = Layer(784, 512, 'softmax')  # Input layer with 784 input neurons and 512 output neurons\n",
    "        Layer2 = Layer(512, 256, 'softmax')  # Hidden layer with 512 input neurons and 256 output neurons\n",
    "        Layer3 = Layer(256, 128, 'softmax')  # Hidden layer with 256 input neurons and 128 output neurons\n",
    "        Layer4 = Layer(128, 10, 'softmax')   # Output layer with 128 input neurons and 10 output neurons (for 10 classes)\n",
    "        \n",
    "        # Append the layers to the list\n",
    "        self.layers.append(Layer1)  \n",
    "        self.layers.append(Layer2)\n",
    "        self.layers.append(Layer3)\n",
    "        self.layers.append(Layer4)\n",
    "        \n",
    "        self.losses = []                     # List to store the loss values during training\n",
    "        self.batch_size = batch_size         # Default batch size for training\n",
    "        self.epochs = epochs                 # Default number of epochs for training\n",
    "        self.learning_rate = learning_rate   # Default learning rate for training\n",
    "        \n",
    "        \n",
    "    def batch_generator(self, df):\n",
    "        # Function to generate batches of data from the DataFrame\n",
    "        for idx in range(0, len(df), self.batch_size):\n",
    "            yield df.iloc[idx : idx + self.batch_size]\n",
    "    \n",
    "    def process_batch(self, batch_df):\n",
    "        # Function to process a batch of data\n",
    "        batch_df.iloc[:, :784] = batch_df.iloc[:, :784] / 255.0   # Normalize the pixel values to [0, 1]\n",
    "        X_batch = batch_df.iloc[:, :784].values             # Shape (batch_size, 784)\n",
    "        X_batch = X_batch.T                                 # Transpose to match the input shape (784, batch_size)\n",
    "        \n",
    "        labels = batch_df['label'].values                   # Shape (batch_size,)\n",
    "        \n",
    "        # Encoding the labels\n",
    "        num_classes = 10\n",
    "        num_examples = len(labels)\n",
    "        Y_batch = np.zeros((num_examples, num_classes))\n",
    "        \n",
    "        for i, label in enumerate(labels):\n",
    "            Y_batch[i, label] = 1\n",
    "            \n",
    "        Y_batch = Y_batch.T                      # Transpose to shape (10, batch_size)\n",
    "        \n",
    "        return X_batch, Y_batch\n",
    "\n",
    "\n",
    "    def train(self, df):\n",
    "        # Training the neural network on the given DataFrame\n",
    "        \n",
    "        for epoch in range(1, self.epochs + 1):\n",
    "            loss = 0\n",
    "            batch_number = 0\n",
    "            \n",
    "            for batch_df in self.batch_generator(df):\n",
    "                X_batch, Y_batch = self.process_batch(batch_df)  # Process the batch of data\n",
    "                \n",
    "                A = X_batch\n",
    "                \n",
    "                for layer in self.layers:\n",
    "                    A = layer.feed_forward(A)\n",
    "                \n",
    "                output = A\n",
    "\n",
    "                loss = np.mean(loss_function('categorical_crossentropy')(Y_batch, output))  # Calculate the loss value using categorical crossentropy\n",
    "                batch_number += 1\n",
    "                dA = loss_derivative('categorical_crossentropy')(Y_batch, output)\n",
    "                \n",
    "                for layer in reversed(self.layers):\n",
    "                    dA = layer.back_propagation(dA)\n",
    "                    \n",
    "                for layer in self.layers:\n",
    "                    layer.W -= self.learning_rate * layer.dW  # Update the weights of the layer\n",
    "                    layer.B -= self.learning_rate * layer.dB  # Update the biases of the layer\n",
    "                \n",
    "            # Average loss over the batches\n",
    "            loss /= batch_number\n",
    "            self.losses.append(loss)                          # Append the loss value to the list of losses\n",
    "            \n",
    "            print( \"Output: \", output)\n",
    "            print( \"Y_batch: \", Y_batch)\n",
    "            \n",
    "            if epoch % 100 == 0:                                # Print the loss value every 100 epochs\n",
    "                print(f\"Epoch {epoch}/{self.epochs}, Loss: {loss * 100:.5f}\")\n",
    "            \n",
    "        plt.plot(range(1, len(self.losses) + 1), self.losses)   # Plot the loss values\n",
    "        plt.title('Loss Value vs Epochs')                       # Set the title of the plot\n",
    "        plt.xlabel('Epochs')                                    # Set the x-axis label\n",
    "        plt.ylabel('Loss Value')                                # Set the y-axis label\n",
    "        plt.grid()                                              # Add grid lines to the plot\n",
    "        plt.show()                                              # Show the plot\n",
    "    \n",
    "    def predict(self, df):\n",
    "        # Function to make predictions on the given DataFrame\n",
    "        df.iloc[:, :784] = df.iloc[:, :784] / 255.0   # Normalize the pixel values to [0, 1]\n",
    "        X_batch = df.iloc[:, :784].values\n",
    "        X_batch = X_batch.T          \n",
    "        \n",
    "        \n",
    "        A = X_batch\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            A = layer.feed_forward(A)\n",
    "            \n",
    "        predictions = np.argmax(A, axis=0)            # Get the index of the maximum value in each column (predicted class)\n",
    "        return predictions            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Network ##\n",
    "Utilising the above defined architecture, we can now use the network to be trained on the MNIST dataset. For the purpose of this project, the entire dataset is used for the training purpose and a separate dataset will be used to test the model accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = NeuralNetwork(epochs=1000, batch_size=32, learning_rate=0.01)  # Create an instance of the NeuralNetwork class\n",
    "nn.train(df)                    # Train the neural network on the DataFrame\n",
    "print(\"Training Completed!\")    # Print a message indicating that the training is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = load_idx_images(r'D:\\Coding\\DSGxRMS\\Artificial Neural Network\\Dataset\\t10k-images.idx3-ubyte')  # Load the MNIST training images\n",
    "labels = load_idx_labels(r'D:\\Coding\\DSGxRMS\\Artificial Neural Network\\Dataset\\t10k-labels.idx1-ubyte')  # Load the MNIST training labels\n",
    "\n",
    "# Creating the Dataframe\n",
    "df = pd.DataFrame(images)  # Create a DataFrame from the images\n",
    "df_labels = pd.DataFrame(labels)  # Create a DataFrame from the labels\n",
    "\n",
    "\n",
    "prediction = nn.predict(df)\n",
    "\n",
    "for label in df_labels.iloc[0:10].values:\n",
    "    print(label)  # Print the first 10 labels from the DataFrame\n",
    "print()\n",
    "print(\"Predictions:\")\n",
    "for i in range(10):\n",
    "    print(prediction[i])  # Print the first 10 predictions from the neural network\n",
    "print()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
