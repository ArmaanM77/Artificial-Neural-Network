{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def activation_function(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'sigmoid'):\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif (function_name.lower() == 'tanh'):\n",
    "            return lambda x: np.tanh(x)\n",
    "        elif (function_name.lower() == 'relu'):\n",
    "            return lambda x: np.maximum(0, x)   \n",
    "        elif (function_name.lower() == 'leaky_relu'):\n",
    "            return lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "        elif (function_name.lower() == 'softmax'):\n",
    "            return lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding activation function: {e}\")\n",
    "        return None\n",
    "        \n",
    "def activation_derivative(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'sigmoid'):\n",
    "            return lambda x: x * (1 - x)\n",
    "        elif (function_name.lower() == 'tanh'):\n",
    "            return lambda x: 1 - np.tanh(x) ** 2\n",
    "        elif (function_name.lower() == 'relu'):\n",
    "            return lambda x: np.where(x > 0, 1, 0)\n",
    "        elif (function_name.lower() == 'leaky_relu'):\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif (function_name.lower() == 'softmax'):\n",
    "            return lambda x: x * (1 - x)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding activation derivative function: {e}\")\n",
    "        return None\n",
    "        \n",
    "        \n",
    "def loss_function(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'mean_squared_error'):\n",
    "            return lambda y_true, y_pred: np.mean((y_true - y_pred) ** 2)\n",
    "        elif (function_name.lower() == 'binary_crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        elif (function_name.lower() == 'categorical_crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.sum(y_true * np.log(y_pred), axis=1).mean()\n",
    "        elif (function_name.lower() == 'crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.sum(y_true * np.log(y_pred), axis=1).mean()\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding loss function: {e}\")\n",
    "        return None\n",
    "\n",
    "def loss_derivative(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'mean_squared_error'):\n",
    "            return lambda y_true, y_pred: 2 * (y_pred - y_true) / y_true.size\n",
    "        elif (function_name.lower() == 'binary_crossentropy'):\n",
    "            return lambda y_true, y_pred: -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        elif (function_name.lower() == 'categorical_crossentropy'):\n",
    "            return lambda y_true, y_pred: -y_true / y_pred\n",
    "        elif (function_name.lower() == 'crossentropy'):\n",
    "            return lambda y_true, y_pred: -y_true / y_pred\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding loss derivative function: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    def __init__(self, input_size, output_size, function_name='sigmoid'):\n",
    "        self.W = np.random.randn(output_size, input_size) * 0.01\n",
    "        self.B = np.zeros((output_size, 1))\n",
    "        self.function_name = function_name\n",
    "        print(\"Weights:\")\n",
    "        print(self.W)\n",
    "        print(\"Biases:\")\n",
    "        print(self.B)\n",
    "            \n",
    "    def feed_forward(self, X):\n",
    "        self.Z = np.dot(self.W, X) + self.B\n",
    "        self.A = activation_function(self.function_name)(self.Z)\n",
    "        return self.A\n",
    "    \n",
    "    def back_propagation(self, dA, X):\n",
    "        m = X.shape[1]\n",
    "        dL = dA * activation_derivative(self.function_name)(self.A)\n",
    "        self.dW = np.dot(dL, X.T) / m\n",
    "        self.dB = np.sum(dL, axis=1, keepdims=True) / m\n",
    "        dX = np.dot(self.W.T, dL)\n",
    "        \n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[-0.00108408  0.01426067 -0.00424903]\n",
      " [ 0.00087429  0.01141518 -0.00560026]]\n",
      "Biases:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "Input Data:\n",
      "[[1]\n",
      " [4]\n",
      " [7]]\n",
      "\n",
      "Required Output\n",
      "[[0]\n",
      " [1]]\n",
      "\n",
      "Forward Output\n",
      "[[0.50655348]\n",
      " [0.50183329]]\n",
      "\n",
      "Loss Value\n",
      "0.25238324606444357\n",
      "\n",
      "Back Output\n",
      "[[-0.00024615]\n",
      " [ 0.00038399]\n",
      " [ 0.00015946]]\n",
      "\n",
      "Forward Output\n",
      "[[2.12342486e-04]\n",
      " [9.99764017e-01]]\n",
      "\n",
      "Loss Value\n",
      "5.0388733531324895e-08\n",
      "\n",
      "Back Output\n",
      "[[-1.27391563e-08]\n",
      " [-5.05591129e-08]\n",
      " [-8.83710214e-08]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "Layer1 = Layer(3,2)\n",
    "input_data = np.array([[1], [4], [7]])\n",
    "true_output = np.array([[0], [1]])\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Input Data:\")\n",
    "print(input_data)\n",
    "print()\n",
    "print(\"Required Output\")\n",
    "print(true_output)\n",
    "print()\n",
    "print(\"Forward Output\")\n",
    "L1 = Layer1.feed_forward(input_data)\n",
    "loss_value = loss_function('mean_squared_error')(true_output, L1)\n",
    "dA = loss_derivative('mean_squared_error')(true_output, L1)\n",
    "\n",
    "print(L1)\n",
    "print()\n",
    "print(\"Loss Value\")\n",
    "print(loss_value)\n",
    "print()\n",
    "print(\"Back Output\")\n",
    "print(Layer1.back_propagation(dA, input_data))\n",
    "print()\n",
    "\n",
    "\n",
    "Layer1.W -= Layer1.dW\n",
    "Layer1.B -= Layer1.dB\n",
    "print(\"Forward Output\")\n",
    "L1 = Layer1.feed_forward(input_data)\n",
    "loss_value = loss_function('mean_squared_error')(true_output, L1)\n",
    "dA = loss_derivative('mean_squared_error')(true_output, L1)\n",
    "\n",
    "print(L1)\n",
    "\n",
    "print()\n",
    "print(\"Loss Value\")\n",
    "print(loss_value)\n",
    "print()\n",
    "print(\"Back Output\")\n",
    "print(Layer1.back_propagation(dA, input_data))\n",
    "print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
