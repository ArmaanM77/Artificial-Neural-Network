{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Neural Network #\n",
    "Full implementation of artificial neurons and MLPs without the use of in built functions or related libraries\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Library Imports ##\n",
    "* Numpy\n",
    "* Pandas\n",
    "* Mathplot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np                         # For Array Operations\n",
    "import pandas as pd                        # For DataFrame Operations\n",
    "import matplotlib.pyplot as plt            # For Plotting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining Global Functions ###\n",
    "| Function Name          | Use                                |\n",
    "| :--------------------: | :--------------------------------: |\n",
    "|activation_function     | Defines few activation functions   |\n",
    "|activation_derivative   | Pre-evaluated derivatives          |\n",
    "|loss_function           | Defines few loss functions         |\n",
    "|loss_derivative         | Pre-evaluated derivatives          |\n",
    "\n",
    "\n",
    "**Available activation functions :**\n",
    "* Sigmoid\n",
    "* Tanh\n",
    "* ReLU\n",
    "* Leaky ReLU\n",
    "* Softmax\n",
    "\n",
    "\n",
    "**Available Loss functions :**\n",
    "* MSE\n",
    "* Binary Cross Entropy\n",
    "* Categorical Cross Entropy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the activation functions and their derivatives\n",
    "def activation_function(function_name):\n",
    "    try:                                                                  \n",
    "        if (function_name.lower() == 'sigmoid'):\n",
    "            return lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif (function_name.lower() == 'tanh'):\n",
    "            return lambda x: np.tanh(x)\n",
    "        elif (function_name.lower() == 'relu'):\n",
    "            return lambda x: np.maximum(0, x)   \n",
    "        elif (function_name.lower() == 'leaky_relu'):\n",
    "            return lambda x: np.where(x > 0, x, 0.01 * x)\n",
    "        elif (function_name.lower() == 'softmax'):\n",
    "            return lambda x: np.exp(x) / np.sum(np.exp(x), axis=0)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding activation function: {e}\")\n",
    "        return None\n",
    "        \n",
    "def activation_derivative(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'sigmoid'):\n",
    "            return lambda x: x * (1 - x)\n",
    "        elif (function_name.lower() == 'tanh'):\n",
    "            return lambda x: 1 - np.tanh(x) ** 2\n",
    "        elif (function_name.lower() == 'relu'):\n",
    "            return lambda x: np.where(x > 0, 1, 0)\n",
    "        elif (function_name.lower() == 'leaky_relu'):\n",
    "            return lambda x: np.where(x > 0, 1, 0.01)\n",
    "        elif (function_name.lower() == 'softmax'):\n",
    "            return lambda x: x * (1 - x)\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding activation derivative function: {e}\")\n",
    "        return None\n",
    "        \n",
    "# Defining the loss functions and their derivatives      \n",
    "def loss_function(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'mean_squared_error'):\n",
    "            return lambda y_true, y_pred: np.mean((y_true - y_pred) ** 2)\n",
    "        elif (function_name.lower() == 'binary_crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.mean(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred))\n",
    "        elif (function_name.lower() == 'categorical_crossentropy'):\n",
    "            return lambda y_true, y_pred: -np.sum(y_true * np.log(y_pred), axis=1).mean()\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding loss function: {e}\")\n",
    "        return None\n",
    "\n",
    "def loss_derivative(function_name):\n",
    "    try:\n",
    "        if (function_name.lower() == 'mean_squared_error'):\n",
    "            return lambda y_true, y_pred: 2 * (y_pred - y_true) / y_true.size\n",
    "        elif (function_name.lower() == 'binary_crossentropy'):\n",
    "            return lambda y_true, y_pred: -(y_true / y_pred) + ((1 - y_true) / (1 - y_pred))\n",
    "        elif (function_name.lower() == 'categorical_crossentropy'):\n",
    "            return lambda y_true, y_pred: -y_true / y_pred\n",
    "        elif (function_name.lower() == 'crossentropy'):\n",
    "            return lambda y_true, y_pred: -y_true / y_pred\n",
    "    except Exception as e:\n",
    "        print(f\"Error finding loss derivative function: {e}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layer Class ###\n",
    "To utilise a deep network of neuron layers, we can define a layer class with variable parameters like, number of neurons in the layer (inputs) and the number of outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer:\n",
    "    # Initializing the layer with weights and biases with a default activation function (Sigmoid)\n",
    "    def __init__(self, input_size, output_size, function_name='sigmoid'):\n",
    "        self.W = np.random.randn(output_size, input_size) * 0.01    # Initializing weights with small random values\n",
    "        self.B = np.zeros((output_size, 1))                         # Initializing biases with zeros\n",
    "        self.function_name = function_name                          # Storing the activation function name\n",
    "        \n",
    "    # Defining the feed forward function for the layer\n",
    "    def feed_forward(self, X):\n",
    "        self.Z = np.dot(self.W, X) + self.B                         # Simply implementing the formula Z = WX + B\n",
    "        self.A = activation_function(self.function_name)(self.Z)    # Applying the activation function to Z to get A\n",
    "        return self.A\n",
    "    \n",
    "    # Defining the back propagation function for the layer\n",
    "    def back_propagation(self, dA, X):\n",
    "        m = X.shape[1]                                              # To utilise batch processing\n",
    "        dL = dA * activation_derivative(self.function_name)(self.A) # Using the chain rule to calculate the derivative of the loss with respect to Z - (dL = dL/dZ)\n",
    "        self.dW = np.dot(dL, X.T) / m                               # Gradient of the loss with respect to W - (dL/dW = dL/dZ * dZ/dW)\n",
    "        self.dB = np.sum(dL, axis=1, keepdims=True) / m             # Gradient of the loss with respect to B - (dL/dB = dL/dZ * dZ/dB)\n",
    "        dX = np.dot(self.W.T, dL)                                   # Gradient of the loss with respect to Input - (dL/dX = dL/dZ * dZ/dX)          \n",
    "        \n",
    "        return dX                                                   # Return the gradient of the loss with respect to the input for the next layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights:\n",
      "[[-0.00265736 -0.02004092  0.016813  ]\n",
      " [ 0.01453095 -0.00515618 -0.00237033]]\n",
      "Biases:\n",
      "[[0.]\n",
      " [0.]]\n",
      "\n",
      "Input Data:\n",
      "[[1]\n",
      " [4]\n",
      " [7]]\n",
      "\n",
      "Required Output\n",
      "[[0]\n",
      " [1]]\n",
      "\n",
      "Forward Output\n",
      "[[0.50871661]\n",
      " [0.49432873]]\n",
      "\n",
      "Loss Value: 0.2572\n",
      "\n",
      "Back Output\n",
      "[[-0.00217459]\n",
      " [-0.00189626]\n",
      " [ 0.00243723]]\n",
      "\n",
      "Forward Output\n",
      "[[2.06801652e-04]\n",
      " [9.99785335e-01]]\n",
      "\n",
      "Loss Value: 0.0000\n",
      "\n",
      "Back Output\n",
      "[[-1.20428216e-08]\n",
      " [-4.56583254e-08]\n",
      " [-7.79900935e-08]]\n",
      "\n",
      "Test Data\n",
      "[[4.85852061e-08]\n",
      " [9.99999948e-01]]\n"
     ]
    }
   ],
   "source": [
    "Layer1 = Layer(3,2)\n",
    "input_data = np.array([[1], [4], [7]])\n",
    "true_output = np.array([[0], [1]])\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"Input Data:\")\n",
    "print(input_data)\n",
    "print()\n",
    "print(\"Required Output\")\n",
    "print(true_output)\n",
    "print()\n",
    "print(\"Forward Output\")\n",
    "L1 = Layer1.feed_forward(input_data)\n",
    "loss_value = loss_function('mean_squared_error')(true_output, L1)\n",
    "dA = loss_derivative('mean_squared_error')(true_output, L1)\n",
    "\n",
    "print(L1)\n",
    "print()\n",
    "\n",
    "print(f\"Loss Value: {loss_value :.4f}\")\n",
    "print()\n",
    "print(\"Back Output\")\n",
    "print(Layer1.back_propagation(dA, input_data))\n",
    "print()\n",
    "\n",
    "\n",
    "Layer1.W -= Layer1.dW\n",
    "Layer1.B -= Layer1.dB\n",
    "print(\"Forward Output\")\n",
    "L1 = Layer1.feed_forward(input_data)\n",
    "loss_value = loss_function('mean_squared_error')(true_output, L1)\n",
    "dA = loss_derivative('mean_squared_error')(true_output, L1)\n",
    "\n",
    "print(L1)\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Loss Value: {loss_value :.4f}\")\n",
    "print()\n",
    "print(\"Back Output\")\n",
    "print(Layer1.back_propagation(dA, input_data))\n",
    "print()\n",
    "\n",
    "\n",
    "test_data = np.array([[2], [8], [14]])\n",
    "L2 = Layer1.feed_forward(test_data)\n",
    "print(\"Test Data\")\n",
    "print(L2)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
